#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
GPU ç¯å¢ƒè¯Šæ–­å·¥å…·

ç”¨äºè¯Šæ–­ä¸ºä»€ä¹ˆ GPU åˆå§‹åŒ–ä¼šè¶…æ—¶æˆ–å¤±è´¥
"""

import sys
import os
from pathlib import Path

def print_section(title):
    """æ‰“å°åˆ†èŠ‚æ ‡é¢˜"""
    print("\n" + "=" * 70)
    print(f"  {title}")
    print("=" * 70 + "\n")

def check_nvidia_driver():
    """æ£€æŸ¥ NVIDIA é©±åŠ¨"""
    print_section("1. NVIDIA é©±åŠ¨æ£€æŸ¥")
    
    try:
        import subprocess
        result = subprocess.run(
            ['nvidia-smi'], 
            capture_output=True, 
            text=True, 
            timeout=5
        )
        
        if result.returncode == 0:
            print("âœ“ NVIDIA é©±åŠ¨å·²å®‰è£…")
            print("\n" + result.stdout)
            return True
        else:
            print("âœ— nvidia-smi å‘½ä»¤å¤±è´¥")
            print(result.stderr)
            return False
            
    except FileNotFoundError:
        print("âœ— nvidia-smi æœªæ‰¾åˆ°")
        print("  NVIDIA é©±åŠ¨å¯èƒ½æœªå®‰è£…")
        return False
    except Exception as e:
        print(f"âœ— æ£€æŸ¥å¤±è´¥: {e}")
        return False

def check_cuda_availability():
    """æ£€æŸ¥ CUDA å¯ç”¨æ€§"""
    print_section("2. CUDA å¯ç”¨æ€§æ£€æŸ¥")
    
    try:
        import onnxruntime as ort
        providers = ort.get_available_providers()
        
        print(f"ONNX Runtime ç‰ˆæœ¬: {ort.__version__}")
        print(f"å¯ç”¨ Providers: {providers}")
        print()
        
        if 'CUDAExecutionProvider' in providers:
            print("âœ“ CUDAExecutionProvider å¯ç”¨")
            return True
        else:
            print("âœ— CUDAExecutionProvider ä¸å¯ç”¨")
            print("  è¿™æ„å‘³ç€ onnxruntime æ£€æµ‹ä¸åˆ° CUDA")
            return False
            
    except ImportError:
        print("âœ— onnxruntime æœªå®‰è£…")
        return False
    except Exception as e:
        print(f"âœ— æ£€æŸ¥å¤±è´¥: {e}")
        return False

def test_simple_cuda_init():
    """æµ‹è¯•ç®€å•çš„ CUDA åˆå§‹åŒ–"""
    print_section("3. ç®€å• CUDA åˆå§‹åŒ–æµ‹è¯•")
    
    try:
        import onnxruntime as ort
        import numpy as np
        
        print("åˆ›å»ºä¸€ä¸ªç®€å•çš„ ONNX Runtime ä¼šè¯ï¼ˆCUDAï¼‰...")
        
        # åˆ›å»ºä¸€ä¸ªæœ€ç®€å•çš„æ¨¡å‹ï¼ˆæ’ç­‰æ˜ å°„ï¼‰
        import onnx
        from onnx import helper, TensorProto
        
        # åˆ›å»ºè¾“å…¥è¾“å‡º
        X = helper.make_tensor_value_info('X', TensorProto.FLOAT, [1, 3])
        Y = helper.make_tensor_value_info('Y', TensorProto.FLOAT, [1, 3])
        
        # åˆ›å»ºæ’ç­‰èŠ‚ç‚¹
        node = helper.make_node('Identity', ['X'], ['Y'])
        
        # åˆ›å»ºå›¾
        graph = helper.make_graph([node], 'test', [X], [Y])
        
        # åˆ›å»ºæ¨¡å‹
        model = helper.make_model(graph)
        
        # ä¿å­˜åˆ°ä¸´æ—¶æ–‡ä»¶
        import tempfile
        with tempfile.NamedTemporaryFile(suffix='.onnx', delete=False) as f:
            onnx.save(model, f.name)
            temp_model_path = f.name
        
        try:
            # å°è¯•ä½¿ç”¨ CUDA Provider åˆ›å»ºä¼šè¯
            print("  ä½¿ç”¨ CUDAExecutionProvider åˆ›å»ºä¼šè¯...")
            session = ort.InferenceSession(
                temp_model_path,
                providers=['CUDAExecutionProvider', 'CPUExecutionProvider']
            )
            
            # æ£€æŸ¥å®é™…ä½¿ç”¨çš„ Provider
            actual_providers = session.get_providers()
            print(f"  å®é™…ä½¿ç”¨çš„ Providers: {actual_providers}")
            
            if 'CUDAExecutionProvider' in actual_providers:
                print("âœ“ CUDA Provider æˆåŠŸæ¿€æ´»")
                
                # å°è¯•è¿è¡Œæ¨ç†
                print("  è¿è¡Œä¸€æ¬¡æ¨ç†æµ‹è¯•...")
                input_data = np.random.randn(1, 3).astype(np.float32)
                output = session.run(None, {'X': input_data})
                print("âœ“ æ¨ç†æˆåŠŸ")
                
                return True
            else:
                print("âš  CUDA Provider æœªæ¿€æ´»ï¼Œé™çº§åˆ° CPU")
                print("  è¿™å¯èƒ½æ˜¯ CUDA ç¯å¢ƒé—®é¢˜")
                return False
                
        finally:
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            os.unlink(temp_model_path)
            
    except ImportError as e:
        print(f"âœ— ç¼ºå°‘å¿…è¦çš„åº“: {e}")
        print("  è¯·å®‰è£…: pip install onnx")
        return False
    except Exception as e:
        print(f"âœ— æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_rapidocr_cpu():
    """æµ‹è¯• RapidOCR CPU æ¨¡å¼"""
    print_section("4. RapidOCR CPU æ¨¡å¼æµ‹è¯•")
    
    try:
        from rapidocr_onnxruntime import RapidOCR
        import tempfile
        from PIL import Image
        import numpy as np
        
        print("åˆå§‹åŒ– RapidOCR (CPU æ¨¡å¼)...")
        ocr = RapidOCR(
            det_use_cuda=False,
            cls_use_cuda=False,
            rec_use_cuda=False
        )
        
        print("âœ“ RapidOCR CPU åˆå§‹åŒ–æˆåŠŸ")
        
        # åˆ›å»ºä¸€ä¸ªç®€å•çš„æµ‹è¯•å›¾ç‰‡
        print("åˆ›å»ºæµ‹è¯•å›¾ç‰‡...")
        img = Image.new('RGB', (100, 100), color='white')
        with tempfile.NamedTemporaryFile(suffix='.png', delete=False) as f:
            img.save(f.name)
            test_img_path = f.name
        
        try:
            print("è¿è¡Œ OCR æµ‹è¯•...")
            result = ocr(test_img_path)
            print("âœ“ OCR è¿è¡ŒæˆåŠŸ")
            print(f"  ç»“æœ: {result}")
            return True
        finally:
            os.unlink(test_img_path)
            
    except ImportError:
        print("âœ— rapidocr_onnxruntime æœªå®‰è£…")
        return False
    except Exception as e:
        print(f"âœ— æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_rapidocr_gpu_with_timeout():
    """æµ‹è¯• RapidOCR GPU æ¨¡å¼ï¼ˆå¸¦è¶…æ—¶ï¼‰"""
    print_section("5. RapidOCR GPU æ¨¡å¼æµ‹è¯•ï¼ˆ10ç§’è¶…æ—¶ï¼‰")
    
    try:
        from rapidocr_onnxruntime import RapidOCR
        import threading
        
        print("å°è¯•åˆå§‹åŒ– RapidOCR (GPU æ¨¡å¼)...")
        print("âš  å¦‚æœå¡ä½è¶…è¿‡ 10 ç§’ï¼Œå°†æ”¾å¼ƒæ­¤æµ‹è¯•")
        
        result_container = {'ocr': None, 'error': None, 'done': False}
        
        def init_gpu():
            try:
                ocr = RapidOCR(
                    det_use_cuda=True,
                    cls_use_cuda=True,
                    rec_use_cuda=True
                )
                result_container['ocr'] = ocr
                result_container['done'] = True
            except Exception as e:
                result_container['error'] = e
                result_container['done'] = True
        
        thread = threading.Thread(target=init_gpu, daemon=True)
        thread.start()
        thread.join(timeout=10)
        
        if thread.is_alive():
            print("âœ— GPU åˆå§‹åŒ–è¶…æ—¶ï¼ˆ10ç§’ï¼‰")
            print("  è¿™æ˜¯é—®é¢˜æ‰€åœ¨ï¼GPU åˆå§‹åŒ–ä¼šå¡ä½")
            print()
            print("å¯èƒ½çš„åŸå› :")
            print("  1. CUDA ç‰ˆæœ¬ä¸åŒ¹é…")
            print("  2. cuDNN åº“åŠ è½½å¤±è´¥")
            print("  3. GPU é©±åŠ¨é—®é¢˜")
            print("  4. æŸäº› CUDA åº“åˆå§‹åŒ–æ—¶æ­»é”")
            print()
            print("å»ºè®®:")
            print("  â€¢ ä½¿ç”¨ CPU æ¨¡å¼ï¼ˆç¨³å®šå¯é ï¼‰")
            print("  â€¢ æˆ–è€…æ›´æ–° NVIDIA é©±åŠ¨")
            return False
        
        if result_container['done']:
            if result_container['error']:
                print(f"âœ— GPU åˆå§‹åŒ–å¤±è´¥: {result_container['error']}")
                return False
            else:
                print("âœ“ GPU åˆå§‹åŒ–æˆåŠŸï¼")
                print("  GPU åŠŸèƒ½åº”è¯¥å¯ä»¥æ­£å¸¸ä½¿ç”¨")
                return True
        else:
            print("âš  åˆå§‹åŒ–çŠ¶æ€æœªçŸ¥")
            return False
            
    except ImportError:
        print("âœ— rapidocr_onnxruntime æœªå®‰è£…")
        return False
    except Exception as e:
        print(f"âœ— æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def main():
    """ä¸»å‡½æ•°"""
    print("\n")
    print("â•”" + "=" * 68 + "â•—")
    print("â•‘" + " " * 20 + "GPU ç¯å¢ƒè¯Šæ–­å·¥å…·" + " " * 24 + "â•‘")
    print("â•š" + "=" * 68 + "â•")
    
    results = []
    
    # 1. æ£€æŸ¥ NVIDIA é©±åŠ¨
    has_nvidia = check_nvidia_driver()
    results.append(("NVIDIA é©±åŠ¨", has_nvidia))
    
    # 2. æ£€æŸ¥ CUDA å¯ç”¨æ€§
    has_cuda = check_cuda_availability()
    results.append(("CUDA å¯ç”¨æ€§", has_cuda))
    
    # 3. ç®€å• CUDA åˆå§‹åŒ–æµ‹è¯•
    if has_cuda:
        cuda_init_ok = test_simple_cuda_init()
        results.append(("ç®€å• CUDA åˆå§‹åŒ–", cuda_init_ok))
    else:
        print_section("3. ç®€å• CUDA åˆå§‹åŒ–æµ‹è¯•")
        print("âŠ˜ è·³è¿‡ï¼ˆCUDA ä¸å¯ç”¨ï¼‰")
        cuda_init_ok = False
    
    # 4. RapidOCR CPU æµ‹è¯•
    cpu_ok = test_rapidocr_cpu()
    results.append(("RapidOCR CPU", cpu_ok))
    
    # 5. RapidOCR GPU æµ‹è¯•
    if has_cuda:
        gpu_ok = test_rapidocr_gpu_with_timeout()
        results.append(("RapidOCR GPU", gpu_ok))
    else:
        print_section("5. RapidOCR GPU æ¨¡å¼æµ‹è¯•")
        print("âŠ˜ è·³è¿‡ï¼ˆCUDA ä¸å¯ç”¨ï¼‰")
        gpu_ok = False
    
    # æ€»ç»“
    print_section("è¯Šæ–­æ€»ç»“")
    
    for name, result in results:
        status = "âœ“ é€šè¿‡" if result else "âœ— å¤±è´¥"
        print(f"{name:<30} {status}")
    
    print()
    
    # ç»™å‡ºå»ºè®®
    print("=" * 70)
    print("  å»ºè®®")
    print("=" * 70)
    print()
    
    if not has_nvidia:
        print("âŒ æœªæ£€æµ‹åˆ° NVIDIA é©±åŠ¨")
        print("   å»ºè®®: å®‰è£…æœ€æ–°çš„ NVIDIA é©±åŠ¨ç¨‹åº")
        print()
    
    if has_nvidia and not has_cuda:
        print("âš  NVIDIA é©±åŠ¨å·²å®‰è£…ï¼Œä½† CUDA ä¸å¯ç”¨")
        print("   å¯èƒ½åŸå› :")
        print("   â€¢ onnxruntime-gpu æœªå®‰è£…")
        print("   â€¢ CUDA ç‰ˆæœ¬ä¸åŒ¹é…")
        print("   å»ºè®®:")
        print("   â€¢ pip install onnxruntime-gpu")
        print()
    
    if has_cuda and not gpu_ok:
        print("âš  CUDA å¯ç”¨ï¼Œä½† RapidOCR GPU åˆå§‹åŒ–å¤±è´¥æˆ–è¶…æ—¶")
        print("   è¿™æ˜¯å½“å‰é—®é¢˜çš„å…³é”®ï¼")
        print()
        print("   å¯èƒ½åŸå› :")
        print("   1. CUDA ç‰ˆæœ¬ä¸ onnxruntime-gpu ä¸åŒ¹é…")
        print("      â€¢ æ‰“åŒ…çš„ç¨‹åºä½¿ç”¨ CUDA 12")
        print("      â€¢ ç”¨æˆ·æœºå™¨çš„ CUDA ç‰ˆæœ¬å¯èƒ½ä¸åŒ")
        print()
        print("   2. cuDNN åº“åŠ è½½é—®é¢˜")
        print("      â€¢ cuDNN ç‰ˆæœ¬ä¸åŒ¹é…")
        print("      â€¢ cuDNN åº“æŸå")
        print()
        print("   3. GPU é©±åŠ¨é—®é¢˜")
        print("      â€¢ é©±åŠ¨ç‰ˆæœ¬è¿‡æ—§")
        print("      â€¢ é©±åŠ¨ä¸ç¨³å®š")
        print()
        print("   âœ… æ¨èè§£å†³æ–¹æ¡ˆ:")
        print("   â€¢ ä½¿ç”¨ CPU æ¨¡å¼ï¼ˆç¨³å®šã€å¿«é€Ÿã€å¯é ï¼‰")
        print("   â€¢ å¯åŠ¨ç¨‹åºæ—¶ä½¿ç”¨ã€Œå¯åŠ¨_CPUæ¨¡å¼.batã€")
        print("   â€¢ æˆ–è®¾ç½®ç¯å¢ƒå˜é‡: set MEMEFINDER_FORCE_CPU=1")
        print()
        print("   ğŸ“Š æ€§èƒ½å¯¹æ¯”:")
        print("   â€¢ å°æ‰¹é‡ (< 500 å¼ ): CPU å’Œ GPU å·®å¼‚ä¸å¤§")
        print("   â€¢ ä¸­æ‰¹é‡ (500-2000 å¼ ): GPU å¿« 2-3 å€")
        print("   â€¢ å¤§æ‰¹é‡ (> 2000 å¼ ): GPU å¿« 3-5 å€")
        print()
    
    if cpu_ok and not gpu_ok:
        print("ğŸ’¡ å¥½æ¶ˆæ¯:")
        print("   CPU æ¨¡å¼å·¥ä½œæ­£å¸¸ï¼")
        print("   æ‚¨å¯ä»¥æ”¾å¿ƒä½¿ç”¨ CPU æ¨¡å¼å¤„ç†å›¾ç‰‡")
        print()

if __name__ == '__main__':
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nâœ— ç”¨æˆ·å–æ¶ˆè¯Šæ–­")
        sys.exit(1)
    except Exception as e:
        print(f"\n\nâœ— è¯Šæ–­å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
    
    input("\næŒ‰å›è½¦é€€å‡º...")
